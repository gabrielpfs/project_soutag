# AnÃ¡lise de Dados Netflix

## ğŸ¯ Contexto do Projeto

O desafio consistiu em transformar dados brutos da Netflix em insights acionÃ¡veis, aplicando tÃ©cnicas de ETL, modelagem de dados, SQL, anÃ¡lise exploratÃ³ria e visualizaÃ§Ã£o de dados. O objetivo era mostrar capacidade ponta a ponta, raciocÃ­nio analÃ­tico estruturado, clareza na documentaÃ§Ã£o e entrega de resultados interpretÃ¡veis para o negÃ³cio.

## Dashboard

![dashboard](assets/step_analysis/dashboard.png)

## 1 Etapa: Banco de Dados (Dataset)

### ğŸ“‚ Acesso e Download do Dataset

Para iniciar o desenvolvimento do teste tÃ©cnico, a primeira aÃ§Ã£o foi acessar o dataset recomendado no enunciado:

- Netflix Movies and TV Shows Dataset (Kaggle)

- Orientacao:

1. Acessar o Link do Kaggle: [Kaggle](https://www.kaggle.com/)
2. Criar uma conta na plataforma
3. Acessar o Dataset: [Netflix Movies and TV Shows â€“ Kaggle](https://www.kaggle.com/datasets/shivamb/netflix-shows)
4. Realizar o Download em: `Download dataset as zip (1 MB).`

- Formato do Dataset: `CSV`.
- NÃºmero de Registros do Dataset: 8.807

### ğŸ“‘ DescriÃ§Ã£o Geral

Este dataset contÃ©m informaÃ§Ãµes detalhadas sobre tÃ­tulos disponÃ­veis na Netflix, incluindo:

- Filmes e sÃ©ries
- PaÃ­s de origem
- Ano de lanÃ§amento
- DuraÃ§Ã£o
- GÃªnero (listado em mÃºltiplas categorias)
- Elenco
- ClassificaÃ§Ã£o indicativa

### ğŸ“‹ Analise da Estrutura das Colunas do Dataset

A seguir, um snapshot com as colunas originais presentes no CSV, seus tipos de dados esperados e uma breve explicaÃ§Ã£o:

![snapshot](assets/step_0/snapshot.png)


### âœ… AÃ§Ãµes Realizadas nesta Etapa

1. Acesso ao link oficial do Kaggle fornecido no enunciado do teste.

2. Download do arquivo CSV contendo a base completa de 8.807 registros.

3. VerificaÃ§Ã£o inicial do formato do arquivo para assegurar compatibilidade com ferramentas de ingestÃ£o (SQL, Python, Power BI).

4. Levantamento da estrutura de colunas do dataset (descriÃ§Ã£o acima) para embasar as prÃ³ximas etapas de modelagem e tratamento.

### ğŸ” ObservaÃ§Ãµes Iniciais

- A base apresenta estrutura em CSV, o que facilita ingestÃ£o tanto em linguagens de programaÃ§Ã£o (Python/R) quanto em bancos de dados SQL.

- O tamanho (8.807 registros) Ã© relativamente pequeno, mas o desafio deve ser tratado como se fossem milhÃµes de registros em constante atualizaÃ§Ã£o (simulaÃ§Ã£o de cenÃ¡rio real de Big Data).

- Futuras etapas incluirÃ£o validaÃ§Ã£o de dados, tratamento de nulos, padronizaÃ§Ã£o de colunas e normalizaÃ§Ã£o.

### Objetivo da Estruturacao e Organizacao de Pastas

A decisÃ£o tÃ©cnica por separar scripts de execuÃ§Ã£o  em(scripts/) das funÃ§Ãµes em (src/) permite modularidade, testes independentes e escalabilidade, importante em cenÃ¡rios de produÃ§Ã£o.

## 2 Etapa â€“ IngestÃ£o de Dados

### ğŸ¯ Objetivo

Realizar o processo de importacao do `Dataset` bruto extraido como Bando de Dados recomendado atraves do Kaggle em formato CSV para o PostgreSQL hospedado na nuvem (Supabase), garantindo:

- PersistÃªncia e Escalabilidade;

- Estrutura de dados inicial para transformaÃ§Ãµes subsequentes;

- ValidaÃ§Ã£o de integridade (colunas e registros).

### âš™ï¸ PreparaÃ§Ã£o do Ambiente

1. ConfiguraÃ§Ã£o do ambiente local de desenvolvimento:

- Editor de cÃ³digo escolhido: Visual Studio Code (VSCode).

- Linguagem de programaÃ§Ã£o principal: Python.

- InstalaÃ§Ã£o das bibliotecas necessÃ¡rias: pandas, sqlalchemy, psycopg2.

- CriaÃ§Ã£o de um ambiente virtual (venv) para isolamento das dependÃªncias.

```shell
python -m venv venv
venv\Scripts\activate # Windows
pip install pandas sqlalchemy psycopg2
pip install psycopg2-binary
```

Python como Ferramenta de ConexÃ£o SQL
Vantagens

1. AutomaÃ§Ã£o: importar CSVs diretamente para PostgreSQL via scripts.

2. Flexibilidade: manipular dados antes de salvar no banco.

3. IntegraÃ§Ã£o: funciona com bibliotecas de anÃ¡lise (pandas, numpy) e visualizaÃ§Ã£o (matplotlib, seaborn, plotly).

4. Escalabilidade: vocÃª consegue criar pipelines de dados que funcionam para milhares ou milhÃµes de registros.

5. Profissional: Ã© uma prÃ¡tica padrÃ£o em Data Science e Analytics.


2. ConfiguraÃ§Ã£o do Banco de Dados em Nuvem:

0. ServiÃ§o escolhido: `Supabase` (`PostgreSQL` hospedado em nuvem).
1. Acessar o Link do Supabase: [Supabase](https://supabase.com/)
2. Criar uma Conta na Plataforma (Gratuito)
3. Criar uma Organizacao e Configurar com Credenciais

- Name
- Type
- Plan

4. Criar um Projeto dentro da Organizacao e Configurar com Credenciais

- Organization
- Project name
- Database password
- Region

5. Apos isso seu Projeto ja estara criado e pronto para adquirir as Credenciais de Conexao.

âš ï¸ Importante: O Supabase gera uma senha padrÃ£o para o usuÃ¡rio postgres. Se vocÃª quiser criar usuÃ¡rios adicionais ou senhas especÃ­ficas, pode fazer isso na aba Authentication > Users ou via SQL.

6. No interior do Project na barra Superior, voce encontra o campo `Connect`.
7. Em Direct Connection voce vai encontrar Credenciais de Conexao

postgresql://postgres:[YOUR-PASSWORD]@yourhost:5432/your_database
- host: your_host
- port: 5432
- database: your_database
- user: your_user

Justificativa da escolha:

- Escalabilidade para bases maiores.
- Facilidade de conexÃ£o remota.
- Interface intuitiva para gestÃ£o de schemas e tabelas.
- Gratuito no plano inicial, adequado para prototipagem.

Vantagens de usar na nuvem:

- NÃ£o precisa se preocupar com instalaÃ§Ã£o, configuraÃ§Ã£o e backup.
- Escalabilidade (cresce conforme aumenta o volume de dados).
- Pode ser acessado de qualquer lugar via internet.
- FÃ¡cil integraÃ§Ã£o com Python, BI e outras ferramentas.

AÃ§Ãµes realizadas:

- CriaÃ§Ã£o da conta na plataforma Supabase.
- CriaÃ§Ã£o de um novo projeto e provisionamento de instÃ¢ncia PostgreSQL.
- Registro de credenciais de conexÃ£o (host, user, password, port, database).

### âœ… AÃ§Ãµes Realizadas nesta Etapa

ConfiguraÃ§Ã£o e validaÃ§Ã£o do ambiente Python no VSCode.

CriaÃ§Ã£o da instÃ¢ncia PostgreSQL em nuvem (Supabase).

ConexÃ£o estabelecida com credenciais seguras.

IngestÃ£o do dataset bruto para a tabela titles_raw no banco de dados.

### ğŸ” ObservaÃ§Ãµes e Boas PrÃ¡ticas

A ingestÃ£o inicial foi realizada na tabela raw, preservando os dados originais sem transformaÃ§Ã£o.

A separaÃ§Ã£o entre camadas (raw, staging, production) serÃ¡ aplicada nas prÃ³ximas etapas de modelagem.

Esta abordagem facilita reprocessamentos e garante governanÃ§a sobre os dados.

### ğŸ“‚ Estrutura de Arquivos Criada

project_soutag_case/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ netflix_titles.csv
â”œâ”€â”€ env/
â”‚ â””â”€â”€ .env
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ run_ingest.py
â”‚ â””â”€â”€ test_connection.py
â””â”€â”€ src/
â”œâ”€â”€ config.py
â”œâ”€â”€ db.py
â”œâ”€â”€ ingest.py
â””â”€â”€ logger.py

- ğŸ” Arquivo .env

```py
DB=your_database
DB_USER=your_database_user
DB_PASSWORD="your_password"
DB_HOST=db.avjflpafvwstzhhmelnn.supabase.co
DB_PORT=5432
DB_NAME=your_database_name
```

### ğŸ“œ Principais Scripts de Suporte

- `config.py` â†’ Carregamento e validaÃ§Ã£o de variÃ¡veis de ambiente (.env).

- `db.py` â†’ CriaÃ§Ã£o do engine de conexÃ£o com PostgreSQL (SQLAlchemy + psycopg2).

- `ingest.py` â†’ FunÃ§Ãµes de ingestÃ£o: leitura do CSV, validaÃ§Ã£o de colunas e carga no banco.

- `logger.py` â†’ ConfiguraÃ§Ã£o de logging estruturado.

- `run_ingest.py` â†’ Script orquestrador que realiza todo o fluxo de ingestÃ£o.

- `test_columns.py` â†’ Testa se as colunas do CSV estÃ£o corretas.

- `test_connection.py` â†’ Testa se a conexÃ£o com PostgreSQL foi estabelecida com sucesso.

### ğŸ”— ConexÃ£o Python â†’ PostgreSQL

```py
from sqlalchemy import create_engine
engine = create_engine(
f"postgresql+psycopg2://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@"
f"{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
)
```

### ğŸš€ ExecuÃ§Ã£o do Script Principal (run_ingest.py)

1. Carrega variÃ¡veis de ambiente.

2. Testa conexÃ£o com PostgreSQL.

3. LÃª o arquivo netflix_titles.csv.

4. Valida colunas obrigatÃ³rias.

5. Ingesta os dados para a tabela netflix_raw.

6. Mostra o total de registros importados e 5 primeiros registros.

Exemplo de saÃ­da esperada:

```shell
2025-09-25 11:00:01 [INFO] CSV loaded: 8807 rows, 12 columns
2025-09-25 11:00:01 [INFO] All expected columns are present.
2025-09-25 11:00:05 [INFO] Data ingested into table 'netflix_raw'
2025-09-25 11:00:05 [INFO] Ingestion completed successfully!
âœ… Total de registros importados: 8807
âœ… Primeiros 5 registros:
('s1', 'Movie', 'Dick Johnson Is Dead', ...)
('s2', 'TV Show', 'Blood & Water', ...)
...
```

Exemplo de saide esperada do test_connection:

```shell
ğŸ”¹ Iniciando teste de conexÃ£o com PostgreSQL...
âœ… VariÃ¡veis de ambiente carregadas com sucesso
âœ… Engine de conexÃ£o criada com sucesso
âœ… ConexÃ£o testada com sucesso! VersÃ£o do PostgreSQL: PostgreSQL 15.3 (ou similar)
```

### âœ… AÃ§Ãµes Realizadas nesta Etapa

- ConfiguraÃ§Ã£o e validaÃ§Ã£o do ambiente Python no VSCode.

- CriaÃ§Ã£o da instÃ¢ncia PostgreSQL em nuvem (Supabase).

- OrganizaÃ§Ã£o modular do projeto com scripts de conexÃ£o, testes e ingestÃ£o.

- IngestÃ£o do dataset bruto para a tabela netflix_raw no banco de dados.

### ğŸ” ObservaÃ§Ãµes e Boas PrÃ¡ticas

- A ingestÃ£o inicial foi realizada na tabela raw, preservando os dados originais sem transformaÃ§Ã£o.

- A separaÃ§Ã£o entre camadas (raw, staging, production) serÃ¡ aplicada nas prÃ³ximas etapas de modelagem.

- UtilizaÃ§Ã£o de .env para proteÃ§Ã£o de credenciais sensÃ­veis.

- ModularizaÃ§Ã£o do cÃ³digo para facilitar manutenÃ§Ã£o, testes e reuso.

## 3 Etapa - Tratamento e Modelagem de Dados

### ğŸ¯ Objetivo

Realizar o tratamento da tabela netflix_raw para gerar tabelas intermediÃ¡rias jÃ¡ limpas, normalizadas e estruturadas, atendendo Ã s boas prÃ¡ticas de modelagem de dados.

### âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

- Editor de desenvolvimento: VSCode.

- Linguagem de programaÃ§Ã£o: Python (utilizando pandas e sqlalchemy).

- Banco de dados: PostgreSQL (Supabase Cloud).

- Estrutura do projeto expandida:

```
project_soutag_case/
â”‚
â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ run_transform.py
â””â”€â”€ src/
â”œâ”€â”€ transform.py
â”œâ”€â”€ config.py
â”œâ”€â”€ db.py
â””â”€â”€ logger.py
```

### ğŸ› ï¸ Principais Processos de TransformaÃ§Ã£o

1. PadronizaÃ§Ã£o de colunas

- ConversÃ£o de nomes para snake_case.

- ConversÃ£o da coluna date_added para tipo DATE.

- SeparaÃ§Ã£o da coluna duration em:

    - duration_value

    - duration_unit

2. Tratamento de valores

- Preenchimento de valores nulos em country com not_specified.

- PadronizaÃ§Ã£o de colunas categÃ³ricas (type, listed_in, rating) em letras minÃºsculas e sem espaÃ§os extras.

- NormalizaÃ§Ã£o da coluna rating, substituindo valores nulos por not_rated.

3. NormalizaÃ§Ã£o de colunas multivalor

- country expandida em tabela auxiliar titles_by_country (relaÃ§Ã£o 1 Ã— N entre show_id e paÃ­ses).

- listed_in expandida em tabela auxiliar titles_by_genre (relaÃ§Ã£o 1 Ã— N entre show_id e gÃªneros).

- RemoÃ§Ã£o de duplicados para integridade relacional.

4. Modelagem relacional

- CriaÃ§Ã£o da tabela principal titles_clean (padronizada).

- CriaÃ§Ã£o de chaves primÃ¡rias em titles_clean(show_id).

- CriaÃ§Ã£o de chaves estrangeiras em titles_by_country e titles_by_genre para garantir integridade referencial.

ğŸ“œ Scripts Desenvolvidos

1. transform.py â†’ ContÃ©m todas as funÃ§Ãµes de transformaÃ§Ã£o, modelagem e normalizaÃ§Ã£o:

- clean_titles(df) â†’ padroniza colunas e trata valores.

- save_clean_table(df) â†’ persiste titles_clean no banco.

- create_primary_key_titles_clean() â†’ adiciona PK.

- create_titles_by_country(df) â†’ gera tabela auxiliar de paÃ­ses.

- create_titles_by_genre(df) â†’ gera tabela auxiliar de gÃªneros.

- create_foreign_keys() â†’ cria FKs relacionando tabelas auxiliares a titles_clean.

- validate_tables() â†’ valida registros, nulos e duplicados.

- run_transform() â†’ orquestra todo o pipeline.

- run_transform.py â†’ Script orquestrador para executar a transformaÃ§Ã£o completa.

### ğŸš€ Fluxo de ExecuÃ§Ã£o do run_transform.py

- Carrega tabela netflix_raw do PostgreSQL.

- Realiza limpeza e padronizaÃ§Ã£o (titles_clean).

- Salva titles_clean no PostgreSQL.

- Cria chave primÃ¡ria na tabela titles_clean.

- Gera tabelas auxiliares titles_by_country e titles_by_genre.

- Cria chaves estrangeiras garantindo integridade referencial.

- Executa validaÃ§Ãµes finais sobre registros, nulos e duplicados.

- Loga primeiros registros das tabelas auxiliares para verificaÃ§Ã£o.

### âœ… AÃ§Ãµes Realizadas nesta Etapa

- ConfiguraÃ§Ã£o do ambiente de transformaÃ§Ã£o no VSCode.

- CriaÃ§Ã£o do script transform.py com funÃ§Ãµes robustas de ETL.

- EstruturaÃ§Ã£o relacional das tabelas intermediÃ¡rias (titles_clean, titles_by_country, titles_by_genre).

- CriaÃ§Ã£o de chaves primÃ¡rias e estrangeiras.

- ImplementaÃ§Ã£o de validaÃ§Ãµes pÃ³s-transformaÃ§Ã£o.

### ğŸ” ObservaÃ§Ãµes e Boas PrÃ¡ticas

- O pipeline foi implementado em Python + SQLAlchemy, favorecendo escalabilidade e manutenÃ§Ã£o.

- UtilizaÃ§Ã£o de logs em cada etapa garante rastreabilidade e auditoria.

- A normalizaÃ§Ã£o em tabelas auxiliares permite anÃ¡lises mais flexÃ­veis e consistentes.

- SeparaÃ§Ã£o clara entre etapas de ingestÃ£o e transformaÃ§Ã£o garante organizaÃ§Ã£o em camadas (raw â†’ clean â†’ analysis).

## 4 Etapa - Analise de Dados

### ğŸ¯ Objetivo

Transformar os dados tratados e normalizados em insights estratÃ©gicos, criando visualizaÃ§Ãµes claras, interpretÃ¡veis e prontas para decisÃµes de negÃ³cio.

Os principais objetivos desta etapa sÃ£o:

- Identificar padrÃµes e tendÃªncias estratÃ©gicas na base de tÃ­tulos da Netflix.

- Criar visualizaÃ§Ãµes de fÃ¡cil compreensÃ£o para stakeholders e BI.

- Gerar insights relevantes para apoiar decisÃµes estratÃ©gicas.

### âš™ï¸ Ferramentas Utilizadas

- Python: Pandas, Matplotlib, Seaborn.

- SQL: queries no PostgreSQL para agregaÃ§Ãµes e filtragens.

- BI: Power BI para dashboards interativos.

### ğŸ“ Planejamento da AnÃ¡lise

Antes de iniciar a execuÃ§Ã£o, definimos as anÃ¡lises prioritÃ¡rias:

1. Top paÃ­ses com mais tÃ­tulos

```sql
SELECT country, COUNT(*) AS total_titles
FROM titles_by_country
GROUP BY country
ORDER BY total_titles DESC
LIMIT 10;
```

VisualizaÃ§Ã£o: grÃ¡fico de barras mostrando os 6 principais paÃ­ses.

2. EvoluÃ§Ã£o de lanÃ§amentos por mÃªs e ano

```py
import pandas as pd
import matplotlib.pyplot as plt


# Dataframe com data_added convertido para datetime
df['date_added'] = pd.to_datetime(df['date_added'])
df.groupby([df['date_added'].dt.year, df['date_added'].dt.month]).size().plot(kind='line')
plt.title('EvoluÃ§Ã£o de lanÃ§amentos por mÃªs-ano')
plt.show()
```

3.DistribuiÃ§Ã£o Filmes x SÃ©ries

```py
df['type'].value_counts().plot(kind='pie', autopct='%1.1f%%')
plt.title('DistribuiÃ§Ã£o de Filmes e SÃ©ries')
plt.show()
```

4. AnÃ¡lise de elenco

- ExplosÃ£o da coluna `cast` em linhas separadas.

- Contagem de apariÃ§Ãµes dos atores/atrizes mais frequentes.

- IdentificaÃ§Ã£o de padrÃµes de colaboraÃ§Ã£o entre atores.

- VisualizaÃ§Ãµes em grÃ¡ficos de barras e redes de colaboraÃ§Ã£o.

5. AnÃ¡lises adicionais sugeridas

- DistribuiÃ§Ã£o por gÃªnero ao longo do tempo.

- CorrelaÃ§Ã£o entre paÃ­s e tipo de tÃ­tulo.

- AnÃ¡lise de ratings e sua relaÃ§Ã£o com duraÃ§Ã£o e paÃ­s.

### ğŸ“ˆ Resultados Esperados

- GrÃ¡ficos claros e interpretÃ¡veis.

- Insights acionÃ¡veis, como paÃ­ses com mais lanÃ§amentos ou tendÃªncias de gÃªnero.

- RelatÃ³rios exploratÃ³rios que possam ser usados para tomada de decisÃ£o.


### âš™ï¸ Fluxo de trabalho - Hibrido

- Python â†’ anÃ¡lise exploratÃ³ria inicial, visualizaÃ§Ãµes rÃ¡pidas, preparaÃ§Ã£o de DataFrames.

- SQL â†’ criaÃ§Ã£o de views e agregaÃ§Ãµes para alimentar Python e Power BI.

- Power BI â†’ dashboards interativos com KPIs estratÃ©gicos.

### ğŸ“ˆ Consultas SQL Principais

1. Top paÃ­ses com mais tÃ­tulos

```sql
CREATE OR REPLACE VIEW view_top_countries AS
SELECT country, COUNT(*) AS total_titles
FROM titles_by_country
GROUP BY country
ORDER BY total_titles DESC;
```

2. EvoluÃ§Ã£o de lanÃ§amentos por mÃªs-ano

```sql
CREATE OR REPLACE VIEW view_monthly_trends AS
SELECT DATE_TRUNC('month', date_added) AS month,
       COUNT(*) AS total_titles
FROM titles_clean
WHERE date_added IS NOT NULL
GROUP BY month
ORDER BY month;
```

3. DistribuiÃ§Ã£o filmes x sÃ©ries

```sql
CREATE OR REPLACE VIEW view_type_distribution AS
SELECT type, COUNT(*) AS total
FROM titles_clean
GROUP BY type;
```

4. Top atores/atrizes

```sql
CREATE OR REPLACE VIEW view_top_cast AS
SELECT actor, COUNT(*) AS appearances
FROM titles_by_cast
GROUP BY actor
ORDER BY appearances DESC
LIMIT 20;
```

5. TÃ­tulos por gÃªnero

```sql
CREATE OR REPLACE VIEW view_titles_by_genre AS
SELECT genre, COUNT(*) AS total_titles
FROM titles_by_genre
GROUP BY genre
ORDER BY total_titles DESC;
```

ğŸ’¡ ObservaÃ§Ã£o: As views criadas permitem alimentar o dashboard e realizar anÃ¡lises repetÃ­veis sem precisar reprocessar o dataset.


#### ğŸ“ˆ AnÃ¡lise em Python

Pipeline sugerido

- Conectar ao PostgreSQL via SQLAlchemy.

- Executar queries SQL e retornar DataFrames.

- Gerar visualizaÃ§Ãµes com Seaborn/Plotly.


```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sqlalchemy import create_engine

# ConexÃ£o com PostgreSQL
engine = create_engine("postgresql+psycopg2://user:password@host:port/dbname")

# Top paÃ­ses
df_countries = pd.read_sql("SELECT * FROM view_top_countries LIMIT 10;", engine)
sns.barplot(data=df_countries, x='total_titles', y='country', palette='viridis')
plt.title("Top 10 PaÃ­ses com Mais TÃ­tulos")
plt.tight_layout()
plt.show()

# EvoluÃ§Ã£o de lanÃ§amentos
df_trends = pd.read_sql("SELECT * FROM view_monthly_trends;", engine)
sns.lineplot(data=df_trends, x='month', y='total_titles', marker='o')
plt.title("EvoluÃ§Ã£o Mensal de LanÃ§amentos")
plt.tight_layout()
plt.show()

# DistribuiÃ§Ã£o filmes x sÃ©ries
df_types = pd.read_sql("SELECT * FROM view_type_distribution;", engine)
plt.pie(df_types['total'], labels=df_types['type'], autopct='%1.1f%%')
plt.title("DistribuiÃ§Ã£o Filmes x SÃ©ries")
plt.show()

# Top atores/atrizes
df_cast = pd.read_sql("SELECT * FROM view_top_cast;", engine)
sns.barplot(data=df_cast, y='actor', x='appearances', palette='magma')
plt.title("Top 20 Atores/Atrizes")
plt.tight_layout()
plt.show()
```

ğŸ’¡ Boas prÃ¡ticas profissionais:

- Modularizar o cÃ³digo (analysis.py) para fÃ¡cil manutenÃ§Ã£o.

- Documentar cada grÃ¡fico e anÃ¡lise no notebook.

- Garantir scripts reprodutÃ­veis e versionÃ¡veis.


### ğŸ“Š PreparaÃ§Ã£o de Dados para Power BI

1. Views SQL â†’ alimentar diretamente as tabelas no Power BI.

2. Relacionamentos:

- titles_clean.show_id â†’ titles_by_country.show_id (1:N)

- titles_clean.show_id â†’ titles_by_genre.show_id (1:N)

- titles_clean.show_id â†’ titles_by_cast.show_id (1:N)

3. KPIs e Medidas sugeridas (DAX):

```m
Total Titles = COUNT(titles_clean[show_id])
```

```m
Total Movies = CALCULATE(COUNT(titles_clean[show_id]), titles_clean[type] = "Movie")
```

```m
Total Series = CALCULATE(COUNT(titles_clean[show_id]), titles_clean[type] = "TV Show")
```

```m
Top Countries = TOPN(10, SUMMARIZE(titles_by_country, titles_by_country[country], "Total", COUNT(titles_by_country[show_id])), [Total], DESC)
```

### ğŸ“Š VisualizaÃ§Ãµes recomendadas:

- Top paÃ­ses â†’ Bar chart horizontal

- EvoluÃ§Ã£o de lanÃ§amentos â†’ Line chart

- DistribuiÃ§Ã£o filmes x sÃ©ries â†’ Pie/Donut chart

- Top atores/atrizes â†’ Bar chart horizontal

- TÃ­tulos por gÃªnero â†’ Treemap ou Column chart

### ğŸ” ObservaÃ§Ãµes Finais da Etapa

A anÃ¡lise combinou SQL para agregaÃ§Ãµes + Python para visualizaÃ§Ã£o + Power BI para dashboards interativos, garantindo performance e clareza.

As visualizaÃ§Ãµes permitem identificar padrÃµes estratÃ©gicos: mercados prioritÃ¡rios, tendÃªncias de lanÃ§amentos, foco do catÃ¡logo e perfis de elenco.

Scripts e queries criados sÃ£o reutilizÃ¡veis, permitindo atualizaÃ§Ã£o periÃ³dica da anÃ¡lise sem retrabalho.
